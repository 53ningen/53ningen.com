---
slug: jjug-2014-12
title: JJUG ナイトセミナー 機械学習・自然言語処理のおはなしを聞きにいった
category: programming
date: 2014-12-18 19:52:35
tags: [機械学習]
pinned: false
---

雑なまとめですが一応公開します．速記なので内容の正確性は保証できません．スライドを見ると良いと思います．

# Javaでカジュアルに機械学習

## 機械学習を始める前に知っておきたいこと

機械学習とは，経験（データ）によって賢くなるアルゴリズムの研究．分類・識別，パターンマイニング・アソシエーションルール，予測・回帰，クラスタリングなどができる．これらは正解があってモデルを作っていく教師あり学習と，正解がない教師なし学習に分類できるそうです．

入力データとしては原則として数値列しか扱えないため，非構造データはそのままでは扱えない．そういった場合はそのデータを表現するなにがしかの「特徴量」を抽出することによって機械学習を実現している．得られた結果が正しいかどうかについて確かめる方法として，k分割交差検証や適合率，再現率，相関係数，決定係数などを用いるものがある．また線形分離という，データを一本の線を轢くことによってデータを分類する方法もある．

## Javaで機械学習

機械学習アルゴリズムのテストはかなり辛く，時間・空間効率の良い実装はさらに難しい．したがって車輪の再発名はなるべく避け既存のライブラリをなるべく使うようにした方がよい．Javaで機械学習を用いたアドホック分析はだいぶ辛いので，そういったことであればPythonを使うと良いとのこと．

Javaで使える機械学習ライブラリのなかで発表者がおすすめなものがliblinear-java．線形分類・回帰可能な問題へ特化している． Wekaは多種多様な機械学習アルゴリズムが提供されているため最初に触るには向いているらしい．MLib(Spark)は分散処理フレームワークSpark上での利用を前提としていて，アドホック分析の環境としてもScalaを用いて利用できる.MahoutはHadoop上の機械学習ライブラリ．Sparkのほうが出てきてから若干オワコン感があるらしい．SAMOAはStomなどの分散ストリーミングフレームワーク上で利用できる機械学習ライブラリだが最近開発があまり活発ではない？Jubatusはリアルタイム性が要求されるときに用いると良い．h2oはディープラーニングをJavaでやりたいときに使うことになるそうです．

UCI Machine Learnigというところで色々なデータが提供されている．だいたいがCSVファイルで提供されている．そのなかでもIrisというのは機械学習の世界でのHello worldにあたるくらい常識的なデータセットらしいです．

Wekaの入力形式はCSVの先頭にちょっとおまけがついているものだが，CSVLoaderというクラスがあるので問題はない．k分割交差検証を実施するweka.classifiers.Evaluation，ロジスティック回帰による分類はweka.classifiers.functions.Logisticあたりを用いる．実際のコードはGitHubにあがっている．

## SparkとMLlibのはなし

機械学習でモデルの制度を高めたいがデータが増えると計算量やIO量が増えるし，そのデータをどこに置いておくのかという点が問題で，かつ従来の機械学習ライブラリは，単一のマシンで計算を行うことが前提としてつくられていた．したがって計算処理の効率は単一のマシンの性能に制約されていた．Hadoopはオープンソースの大規模分散処理基盤で特別な聞きを用いずコモディティなサーバ機器を複数束ねてクラスタを形成して，並列分散処理を可能にするものだそうです．

HDFSは複数のスレーブを束ね，大きなファイルシステムに見せる仕組み，故障することが前提の設計．MapReduceは大規模分散処理向けのフレームワークでこちらも故障が前提．これらを連携させることにより真価を発揮する．HadoopとMahoutの登場で機械学習はある側面でスケーラブルになったそうな．MapReduceを前提としていると反復処理の回数を増やすほど，モデルが作られるまでのレイテンシが大きくなる．Hadoopはスループットを最大にすることを目的にしているため，ジョブが多段になったときにもオーバーヘッドが問題となっていく．

Apache Sparkとはスループットとレイテンシの両方が必要な問題領域にアプローチするために開発されたOSSのインメモリ分散処理基盤．HadoopはMapReduceという単一の処理パラダイムであるが，RDDという異なる処理モデルがとられている．

HadoopはMapとReduceでジョブを構成している．SparkはRDDで処理をしたデータをチェーンでつなげた全体を単一ジョブとして構成する． Sparkの目標の一つはコアとなる分散処理エンジンを中心に据え，それを活用するためのライブラリを充実させることで，そのひとつがMLlibになる．Scala/Java/Pythonでコードを書ける．

# 自然言語処理のはなし 

このあたりで動画あがってるのに気付いたのでメモしてない

# 感想

ElasticSarchとKibanaとMLlibあたり面白そうなので、それらを使って何か作ってみようと思いました
