---
title: JJUG ナイトセミナー 機械学習・自然言語処理のおはなしを聞きにいった
category: programming
date: 2014-12-18 19:52:35
tags: [機械学習]
pinned: false
---

雑なまとめですが一応公開します．速記なので内容の正確性は保証できません．スライドを見ると良いと思います．

# Java でカジュアルに機械学習

## 機械学習を始める前に知っておきたいこと

機械学習とは，経験（データ）によって賢くなるアルゴリズムの研究．分類・識別，パターンマイニング・アソシエーションルール，予測・回帰，クラスタリングなどができる．これらは正解があってモデルを作っていく教師あり学習と，正解がない教師なし学習に分類できるそうです．

入力データとしては原則として数値列しか扱えないため，非構造データはそのままでは扱えない．そういった場合はそのデータを表現するなにがしかの「特徴量」を抽出することによって機械学習を実現している．得られた結果が正しいかどうかについて確かめる方法として，k 分割交差検証や適合率，再現率，相関係数，決定係数などを用いるものがある．また線形分離という，データを一本の線を轢くことによってデータを分類する方法もある．

## Java で機械学習

機械学習アルゴリズムのテストはかなり辛く，時間・空間効率の良い実装はさらに難しい．したがって車輪の再発名はなるべく避け既存のライブラリをなるべく使うようにした方がよい．Java で機械学習を用いたアドホック分析はだいぶ辛いので，そういったことであれば Python を使うと良いとのこと．

Java で使える機械学習ライブラリのなかで発表者がおすすめなものが liblinear-java．線形分類・回帰可能な問題へ特化している． Weka は多種多様な機械学習アルゴリズムが提供されているため最初に触るには向いているらしい．MLib(Spark)は分散処理フレームワーク Spark 上での利用を前提としていて，アドホック分析の環境としても Scala を用いて利用できる.Mahout は Hadoop 上の機械学習ライブラリ．Spark のほうが出てきてから若干オワコン感があるらしい．SAMOA は Stom などの分散ストリーミングフレームワーク上で利用できる機械学習ライブラリだが最近開発があまり活発ではない？Jubatus はリアルタイム性が要求されるときに用いると良い．h2o はディープラーニングを Java でやりたいときに使うことになるそうです．

UCI Machine Learnig というところで色々なデータが提供されている．だいたいが CSV ファイルで提供されている．そのなかでも Iris というのは機械学習の世界での Hello world にあたるくらい常識的なデータセットらしいです．

Weka の入力形式は CSV の先頭にちょっとおまけがついているものだが，CSVLoader というクラスがあるので問題はない．k 分割交差検証を実施する weka.classifiers.Evaluation，ロジスティック回帰による分類は weka.classifiers.functions.Logistic あたりを用いる．実際のコードは GitHub にあがっている．

## Spark と MLlib のはなし

機械学習でモデルの制度を高めたいがデータが増えると計算量や IO 量が増えるし，そのデータをどこに置いておくのかという点が問題で，かつ従来の機械学習ライブラリは，単一のマシンで計算を行うことが前提としてつくられていた．したがって計算処理の効率は単一のマシンの性能に制約されていた．Hadoop はオープンソースの大規模分散処理基盤で特別な聞きを用いずコモディティなサーバ機器を複数束ねてクラスタを形成して，並列分散処理を可能にするものだそうです．

HDFS は複数のスレーブを束ね，大きなファイルシステムに見せる仕組み，故障することが前提の設計．MapReduce は大規模分散処理向けのフレームワークでこちらも故障が前提．これらを連携させることにより真価を発揮する．Hadoop と Mahout の登場で機械学習はある側面でスケーラブルになったそうな．MapReduce を前提としていると反復処理の回数を増やすほど，モデルが作られるまでのレイテンシが大きくなる．Hadoop はスループットを最大にすることを目的にしているため，ジョブが多段になったときにもオーバーヘッドが問題となっていく．

Apache Spark とはスループットとレイテンシの両方が必要な問題領域にアプローチするために開発された OSS のインメモリ分散処理基盤．Hadoop は MapReduce という単一の処理パラダイムであるが，RDD という異なる処理モデルがとられている．

Hadoop は Map と Reduce でジョブを構成している．Spark は RDD で処理をしたデータをチェーンでつなげた全体を単一ジョブとして構成する． Spark の目標の一つはコアとなる分散処理エンジンを中心に据え，それを活用するためのライブラリを充実させることで，そのひとつが MLlib になる．Scala/Java/Python でコードを書ける．

# 自然言語処理のはなし

このあたりで動画あがってるのに気付いたのでメモしてない

# 感想

ElasticSarch と Kibana と MLlib あたり面白そうなので、それらを使って何か作ってみようと思いました
